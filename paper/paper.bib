
@online{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-10-17},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2020-09-18},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/awsm/Zotero/storage/ZGYRSZS2/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf}
}

@article{butlerMachineLearningMolecular2018,
  title = {Machine Learning for Molecular and Materials Science},
  author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  date = {2018-07},
  journaltitle = {Nature},
  volume = {559},
  number = {7715},
  pages = {547--555},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0337-2},
  url = {https://www.nature.com/articles/s41586-018-0337-2},
  urldate = {2020-09-29},
  abstract = {Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.},
  langid = {english},
  file = {/home/awsm/Zotero/storage/5PBC4RBC/Butler et al. - 2018 - Machine learning for molecular and materials scien.pdf}
}

@article{chenGraphNetworksUniversal2019,
  ids = {chenGraphNetworksUniversal2019a},
  title = {Graph {{Networks}} as a {{Universal Machine Learning Framework}} for {{Molecules}} and {{Crystals}}},
  author = {Chen, Chi and Ye, Weike and Zuo, Yunxing and Zheng, Chen and Ong, Shyue Ping},
  date = {2019-05-14},
  journaltitle = {Chemistry of Materials},
  shortjournal = {Chem. Mater.},
  volume = {31},
  number = {9},
  pages = {3564--3572},
  publisher = {{American Chemical Society}},
  issn = {0897-4756},
  doi = {10.1021/acs.chemmater.9b01294},
  url = {https://doi.org/10.1021/acs.chemmater.9b01294},
  urldate = {2020-09-18},
  abstract = {Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on ∼60 000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps, and elastic moduli of crystals, achieving better than density functional theory accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy, and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure, and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).},
  file = {/home/awsm/Zotero/storage/ELIBTF2Q/Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf;/home/awsm/Zotero/storage/YLE3IXD2/Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf}
}

@online{dillonTensorFlowDistributions2017,
  ids = {dillonTensorFlowDistributions2017a},
  title = {{{TensorFlow Distributions}}},
  author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  date = {2017-11-28},
  eprint = {1711.10604},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.10604},
  urldate = {2020-10-07},
  abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  file = {/home/awsm/Zotero/storage/BSR26DLG/Dillon et al. - 2017 - TensorFlow Distributions.pdf}
}

@article{dunnBenchmarkingMaterialsProperty2020,
  title = {Benchmarking Materials Property Prediction Methods: The {{Matbench}} Test Set and {{Automatminer}} Reference Algorithm},
  shorttitle = {Benchmarking Materials Property Prediction Methods},
  author = {Dunn, Alexander and Wang, Qi and Ganose, Alex and Dopp, Daniel and Jain, Anubhav},
  date = {2020-09-15},
  journaltitle = {npj Computational Materials},
  volume = {6},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2057-3960},
  doi = {10.1038/s41524-020-00406-3},
  url = {https://www.nature.com/articles/s41524-020-00406-3},
  urldate = {2020-10-28},
  abstract = {We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13\,ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a material’s composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark. We also show our test suite is capable of exposing predictive advantages of each algorithm—namely, that crystal graph methods appear to outperform traditional machine learning methods given \textasciitilde 104 or greater data points. We encourage evaluating materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.},
  issue = {1},
  langid = {english},
  file = {/home/awsm/Zotero/storage/HWEE87Q8/Dunn et al. - 2020 - Benchmarking materials property prediction methods.pdf}
}

@online{hensmanGaussianProcessesBig2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  date = {2013-09-26},
  eprint = {1309.6835},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1309.6835},
  urldate = {2021-09-02},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/awsm/Zotero/storage/EVQHP4JV/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf;/home/awsm/Zotero/storage/PPAZUPMB/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf}
}

@article{kimActivelearningMaterialsDesign2019,
  title = {Active-Learning and Materials Design: The Example of High Glass Transition Temperature Polymers},
  shorttitle = {Active-Learning and Materials Design},
  author = {Kim, Chiho and Chandrasekaran, Anand and Jha, Anurag and Ramprasad, Rampi},
  date = {2019-09},
  journaltitle = {MRS Communications},
  volume = {9},
  number = {3},
  pages = {860--866},
  publisher = {{Cambridge University Press}},
  issn = {2159-6859, 2159-6867},
  doi = {10.1557/mrc.2019.78},
  url = {https://www.cambridge.org/core/journals/mrs-communications/article/activelearning-and-materials-design-the-example-of-high-glass-transition-temperature-polymers/853CA8B09BD8AB02160E03CAFB32730A},
  urldate = {2020-11-23},
  abstract = {, Machine-learning (ML) approaches have proven to be of great utility in modern materials innovation pipelines. Generally, ML models are trained on predetermined past data and then used to make predictions for new test cases. Active-learning, however, is a paradigm in which ML models can direct the learning process itself through providing dynamic suggestions/queries for the “next-best experiment.” In this work, the authors demonstrate how an active-learning framework can aid in the discovery of polymers possessing high glass transition temperatures (Tg). Starting from an initial small dataset of polymer Tg measurements, the authors use Gaussian process regression in conjunction with an active-learning framework to iteratively add Tg measurements of candidate polymers to the training dataset. The active-learning framework employs one of three decision making strategies (exploitation, exploration, or balanced exploitation/exploration) for selection of the “next-best experiment.” The active-learning workflow terminates once 10 polymers possessing a Tg greater than a certain threshold temperature are selected. The authors statistically benchmark the performance of the aforementioned three strategies (against a random selection approach) with respect to the discovery of high-Tg polymers for this particular demonstrative materials design challenge.},
  langid = {english},
  file = {/home/awsm/Zotero/storage/PBJBVBEW/Kim et al. - 2019 - Active-learning and materials design the example .pdf}
}

@article{ramprasadMachineLearningMaterials2017,
  title = {Machine Learning in Materials Informatics: Recent Applications and Prospects},
  shorttitle = {Machine Learning in Materials Informatics},
  author = {Ramprasad, Rampi and Batra, Rohit and Pilania, Ghanshyam and Mannodi-Kanakkithodi, Arun and Kim, Chiho},
  date = {2017-12-13},
  journaltitle = {npj Computational Materials},
  volume = {3},
  number = {1},
  pages = {1--13},
  issn = {2057-3960},
  doi = {10.1038/s41524-017-0056-5},
  url = {https://www.nature.com/articles/s41524-017-0056-5},
  urldate = {2020-09-29},
  abstract = {Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods—due to the cost, time or effort involved—but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints, also referred to as “descriptors”, may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative—extending into new materials spaces—provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven “materials informatics” strategies undertaken in the last decade, with particular emphasis on the fingerprint or descriptor choices. The review also identifies some challenges the community is facing and those that should be overcome in the near future.},
  langid = {english}
}

@article{schmidtRecentAdvancesApplications2019,
  ids = {schmidtRecentAdvancesApplications2019a},
  title = {Recent Advances and Applications of Machine Learning in Solid-State Materials Science},
  author = {Schmidt, Jonathan and Marques, Mário R. G. and Botti, Silvana and Marques, Miguel A. L.},
  date = {2019-08-08},
  journaltitle = {npj Computational Materials},
  volume = {5},
  number = {1},
  pages = {1--36},
  publisher = {{Nature Publishing Group}},
  issn = {2057-3960},
  doi = {10.1038/s41524-019-0221-0},
  url = {https://www.nature.com/articles/s41524-019-0221-0},
  urldate = {2020-09-30},
  abstract = {One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure–property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.},
  issue = {1},
  langid = {english},
  file = {/home/awsm/Zotero/storage/PFW9WKG6/Schmidt et al. - 2019 - Recent advances and applications of machine learni.pdf}
}

@online{tranMethodsComparingUncertainty2020,
  title = {Methods for Comparing Uncertainty Quantifications for Material Property Predictions},
  author = {Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W.},
  date = {2020-02-20},
  eprint = {1912.10066},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics},
  url = {http://arxiv.org/abs/1912.10066},
  urldate = {2020-09-20},
  abstract = {Data science and informatics tools have been proliferating recently within the computational materials science and catalysis fields. This proliferation has spurned the creation of various frameworks for automated materials screening, discovery, and design. Underpinning these frameworks are surrogate models with uncertainty estimates on their predictions. These uncertainty estimates are instrumental for determining which materials to screen next, but the computational catalysis field does not yet have a standard procedure for judging the quality of such uncertainty estimates. Here we present a suite of figures and performance metrics derived from the machine learning community that can be used to judge the quality of such uncertainty estimates. This suite probes the accuracy, calibration, and sharpness of a model quantitatively. We then show a case study where we judge various methods for predicting density-functional-theory-calculated adsorption energies. Of the methods studied here, we find that the best performer is a model where a convolutional neural network is used to supply features to a Gaussian process regressor, which then makes predictions of adsorption energies along with corresponding uncertainty estimates.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Materials Science,Physics - Computational Physics}
}

@article{xieCrystalGraphConvolutional2018,
  title = {Crystal {{Graph Convolutional Neural Networks}} for an {{Accurate}} and {{Interpretable Prediction}} of {{Material Properties}}},
  author = {Xie, Tian and Grossman, Jeffrey C.},
  date = {2018-04-06},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {120},
  number = {14},
  pages = {145301},
  doi = {10.1103/PhysRevLett.120.145301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301},
  urldate = {2020-09-29},
  abstract = {The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 104 data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.},
  file = {/home/awsm/Zotero/storage/P9DXDMJT/Xie and Grossman - 2018 - Crystal Graph Convolutional Neural Networks for an.pdf}
}

@article{xueAcceleratedSearchMaterials2016,
  title = {Accelerated Search for Materials with Targeted Properties by Adaptive Design},
  author = {Xue, Dezhen and Balachandran, Prasanna V. and Hogden, John and Theiler, James and Xue, Deqing and Lookman, Turab},
  date = {2016-04-15},
  journaltitle = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {11241},
  issn = {2041-1723},
  doi = {10.1038/ncomms11241},
  url = {https://www.nature.com/articles/ncomms11241},
  urldate = {2020-09-29},
  abstract = {Finding new materials with targeted properties has traditionally been guided by intuition, and trial and error. With increasing chemical complexity, the combinatorial possibilities are too large for an Edisonian approach to be practical. Here we show how an adaptive design strategy, tightly coupled with experiments, can accelerate the discovery process by sequentially identifying the next experiments or calculations, to effectively navigate the complex search space. Our strategy uses inference and global optimization to balance the trade-off between exploitation and exploration of the search space. We demonstrate this by finding very low thermal hysteresis (ΔT) NiTi-based shape memory alloys, with Ti50.0Ni46.7Cu0.8Fe2.3Pd0.2 possessing the smallest ΔT (1.84 K). We synthesize and characterize 36 predicted compositions (9 feedback loops) from a potential space of ∼800,000 compositions. Of these, 14 had smaller ΔT than any of the 22 in the original data set.},
  langid = {english}
}


